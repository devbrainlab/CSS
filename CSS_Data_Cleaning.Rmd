---
title: "CSS_Data_Import"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
```

```{r Load Required Packages, message=FALSE, warning=FALSE, include=FALSE}
## Load required packages ##
packages <-  c("tidyverse",
               "reshape2",
               "nlme", "lme4",
               "data.table", "psych",
               "parallel","lubridate",
               "mgcv", "ggpubr", "broom", 
               "table1", "apaTables", "readxl", "ggalluvial",
               "mlVAR")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}
lapply(packages, library, character.only = TRUE)


theme_kate <- function () { 
  theme_bw() +
    theme_minimal(base_size = 14, base_family = "Avenir") +
    theme(axis.line = element_line(colour = "black"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank(),
          legend.position="none")
}
```

Set root path for Lucy
```{r}
root_path="/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/"
```


Import data files
Don't run! You don't need to.
```{r eval=FALSE, include=FALSE}
#read in Qualtrics data
qualtrics <- list.files(path="/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/Qualtrics", full.names = TRUE) %>% #strings as characters = false
  lapply(read_excel) %>% #change to read csv, parse by/str split, take first character, keep full date in case we need it later
  bind_rows %>%
  select(1:2,7,9,27:96, 137, 142, 143)

#Convert start dates & end dates to proper format
qualtrics$StartDate <- as.Date(as.numeric(qualtrics$StartDate), origin="1899-12-30")
qualtrics$EndDate <- as.Date(as.numeric(qualtrics$EndDate), origin="1899-12-30")

#Convert to numeric (NOTE: Need to do this to other columns as well, figure out which ones & potentially do this on import)
qualtrics$wave <- as.numeric(qualtrics$wave)
#Replace NA values w/ 1 (no wave specified first week)
qualtrics$wave[is.na(qualtrics$wave)] <- 1


#read in RedCap data
redcap <- read.csv("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/RedCap/Covid19SocialScreens_DATA_2020-11-18_2325.csv") %>%
  dplyr::rename(ResponseId=qualtrics_id)

#Join Qualtrics and Redcap data, sort by ID and wave
all_data <- left_join(qualtrics, redcap, by="ResponseId") %>%
  arrange(record_id, wave)
#Rearrange columns to bring record id and wave to the beginning
all_data <- all_data[,c(78, 77, 1:76, 79:ncol(all_data))]

save(all_data, file="all_data.Rda")


#Read in Network Canvas Data wave by wave, add columns for filename(ID) and wave #
datalist = list()
for (i in 1:10) {
   test <- list.files(path = paste0("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/NetCanvasCSV/Wave", i),
           pattern="*.csv", 
           full.names = T) %>% 
     map_df(function(x) read_csv(x, col_types = cols(.default = "c")) %>%
            mutate(ResponseId=gsub(".csv","",basename(x)))%>%
              mutate(wave=i)
            )
   datalist[[i]] <- test
}
netcanvas <- dplyr::bind_rows(datalist)
save(netcanvas, file="netcanvas.Rda")
```

Load Saved Data
```{r}
#' *Run to load full data * 
load("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/all_data.Rda")
load("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/netcanvas.Rda")
load("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/netcanvas_cleaned.Rda")


#' *Run to load processed data*
load("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/processed_data.Rda")

#' *Run to load data for pre-reg analysis*
load("/Volumes/devbrainlab/Covid19SocialScreens/CSS_Data/analysis_data.Rda")
```

Calculate Summary Stats
```{r}

#calculate mean age of participants that completed 2+ waves

age_calc <- qualtrics %>%
  select("age_years", "age_months", "wave") %>%
  filter(wave==2) %>%
  add_row("age_years" = "18", "age_months"= "5", "wave"= 6)

age_calc$age_years <- as.numeric(age_calc$age_years)
age_calc$age_months <- as.numeric(age_calc$age_months)

age_calc$years_in_months <- age_calc$age_years*12 
age_calc$age_in_months <- age_calc$years_in_months + age_calc$age_months#rowSums(age_calc[2,4])

mean_age <- mean(age_calc$age_in_months)
sd_age <- sd(age_calc$age_in_months)

```

Calculate Sum Scores + Subscores
```{r}

#create copy for testing
data_copy <- data.frame(all_data)

#Get month for start/end date
data_copy$StartMonth <- months(data_copy$StartDate)
data_copy$EndMonth <- months(data_copy$EndDate)

#Recode YAM-5
data_copy <- data_copy %>%
  mutate_at(vars(starts_with("yam")),
  .funs=funs(recode(.,"Never"=0, "Sometimes"=1, "Often"=2, "Always"=3)))  #Need to figure out how to add default
#sum scores for YAM-5
data_copy <- data_copy %>% mutate(yam_total = rowSums(.[grep("yam", names(.))], na.rm = T))  #remove NAs - make sure to account for this

#NOTE: Selective mutism subscale was not included in questions- question numbers have been shifted accordingly (may be different than what is stated online)
#Subscores for YAM-5: sum and divide by number of completed items
#Separation Anxiety:
data_copy <- data_copy %>% mutate(yam_separation = rowSums(.[c("yam_1", "yam_5", "yam_9", "yam_13", "yam_17", "yam_21")], na.rm = T))

#Mutism was excluded from scale
#Selective Mutism: 2,11,20,25    

#Social Anxiety: 3,7,12,16,23,28
data_copy <- data_copy %>% mutate(yam_social = rowSums(.[c("yam_2", "yam_6", "yam_10", "yam_14", "yam_20", "yam_24")], na.rm = T))
#Panic Disorder:4,8,13,17,21,26
data_copy <- data_copy %>% mutate(yam_panic = rowSums(.[c("yam_3", "yam_7", "yam_11", "yam_15", "yam_18", "yam_22")], na.rm = T))
#Generalized Anxiety:5,9,14,18,22,27
data_copy <- data_copy %>% mutate(yam_general = rowSums(.[c("yam_4", "yam_8", "yam_12", "yam_16", "yam_19", "yam_23")], na.rm = T))

# Calculate how many YAM items were left unanswered, exclude if <80%
data_copy <- data_copy %>% mutate(yam_na = rowSums(is.na(.[grep("yam", names(.))]), na.rm = T))
#if >=5 items left blank, exclude

yam_exclusion_indices <- which(data_copy$yam_na >= 5)  #indices of participants excluded for >= 5 NAs
yam_exclusion_ids <- data_copy$ResponseID[yam_exclusion_indices] #ids of excluded participants

#If needed for an analysis, exclude participants
#yam_analysis <- subset(data_copy, (!(ResponseID %in% yam_exclusion_ids)))

#Recode MSPSS
data_copy <- data_copy %>%
  mutate_at(vars(starts_with("mspss")),
  .funs=funs(recode(.,"Very Strongly Disagree"=1, "Strongly Disagree"=2, "Mildly Disagree"=3, "Neutral"=4, "Mildly Agree"=5, "Strongly Agree"=6, "Very Strongly Agree"=7))) 


#sum scores for MSPSS
data_copy <- data_copy %>% mutate(mspss_total = rowSums(.[grep("mspss", names(.))], na.rm = T)/12)

#Subscores for MSPSS
# Significant Other: 1,2,5,10 (sum & divide by 4)
data_copy <- data_copy %>% mutate(mspss_sigother = rowSums(.[c("mspss_1", "mspss_2", "mspss_5", "mspss_10")], na.rm = T)/(4-rowSums(is.na(.[c("mspss_1", "mspss_2", "mspss_5", "mspss_10")]))))
# Family: 3,4,8,11 (sum & divide by 4)
data_copy <- data_copy %>% mutate(mspss_family = rowSums(.[c("mspss_3", "mspss_4", "mspss_8", "mspss_11")], na.rm = T)/(4-rowSums(is.na(.[c("mspss_3", "mspss_4", "mspss_8", "mspss_11")]))))
# Friends: 6,7,9,12 (sum & divide by 4)
data_copy <- data_copy %>% mutate(mspss_friends = rowSums(.[c("mspss_6", "mspss_7", "mspss_9", "mspss_12")], na.rm = T)/(4-rowSums(is.na(.[c("mspss_6", "mspss_7", "mspss_9", "mspss_12")]))))

# Calculate how many MSPSS items were finished, exclude if <80%
data_copy <- data_copy %>% mutate(mspss_na = rowSums(is.na(.[grep("mspss", names(.))]), na.rm = T))
#If >= 3 items left blank, exclude

mspss_exclusion_indices <- which(data_copy$mspss_na >= 3)  #indices of participants excluded for >- 3 NAs
mspss_exclusion_ids <- data_copy$ResponseID[mspss_exclusion_indices] #ids of excluded participants

#If needed for an analysis, exclude participants
#mspss_analysis <- subset(data_copy, (!(ResponseID %in% mspss_exclusion_ids)))
```

Data Viz for YAM & MSPSS
```{r}
#box plot for yam by wave
ggboxplot(data_copy, x = "wave", y = "yam_total", color="wave")
ggboxplot(data_copy, x = "wave", y = "yam_separation", color="wave")
ggboxplot(data_copy, x = "wave", y = "yam_social", color="wave")
ggboxplot(data_copy, x = "wave", y = "yam_panic", color="wave")
ggboxplot(data_copy, x = "wave", y = "yam_general", color="wave")


#box plot for mspss by wave
ggboxplot(data_copy, x = "wave", y = "mspss_total", color="wave")
ggboxplot(data_copy, x = "wave", y = "mspss_sigother", color="wave")
ggboxplot(data_copy, x = "wave", y = "mspss_family", color="wave")
ggboxplot(data_copy, x = "wave", y = "mspss_friends", color="wave")


```

Screentime
```{r}
# For each wave, check if screentime is complete, search app_category_this_week_[1-5] for social or Social, find matching time
#screentime_complete==1 & ifelse(grepl("social"|"Social", data_copy$[app_category_this_week[1-5]]), "yes", "no")

#'*Get social screen use THIS WEEK*

#Convert format to long (category names)
socialmediathisweek<-data_copy %>%
  select(wave, ResponseId,contains("app_category_this_week_")) %>% 
  gather(key = category, value=app,3:7)


#Convert format to long (category time), change values to match categories above
socialmediatimethisweek<-data_copy %>%
  select(wave, ResponseId,contains("app_category_time_this_week_")) %>% 
  gather(key=category,value=socialtime,3:7) %>%
  mutate(category = case_when(
    category=="app_category_time_this_week_1" ~ "app_category_this_week_1",
    category=="app_category_time_this_week_2" ~ "app_category_this_week_2",
    category=="app_category_time_this_week_3" ~ "app_category_this_week_3",
    category=="app_category_time_this_week_4" ~ "app_category_this_week_4",
    category=="app_category_time_this_week_5" ~ "app_category_this_week_5"
  ))

#Join to get time per category, filter out non social networking values
socialuse<-left_join(socialmediathisweek, socialmediatimethisweek,by=c("wave","ResponseId", "category")) %>%
  filter(app=="Social Networking" | app=="social networking") 


#Convert time to numeric value (i.e. 2:04 to 124 minutes)
  socialuse$socialtimemin = as.numeric(unlist(lapply(socialuse$socialtime, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(socialuse$socialtime, function (x) strsplit(x, ":")[[1]][2])))
  
  
#All Screen Use:
  data_copy$tot_time_min = as.numeric(unlist(lapply(data_copy$tot_time_this_week, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(data_copy$tot_time_this_week, function (x) strsplit(x, ":")[[1]][2])))
  
#Average Use
    data_copy$avg_time_min = as.numeric(unlist(lapply(data_copy$avg_time_this_week, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(data_copy$avg_time_this_week, function (x) strsplit(x, ":")[[1]][2])))
    
    
#Add tiktok to social use
    #if tiktok in most used apps, extract matching time [top_app_name_this_week], [app_time_by_name]
    #add to social use
    
    tiktok_thisweek<-data_copy %>%
  select(wave, ResponseId,contains("top_app_name_this_week_")) %>% 
  gather(key = category, value=app,3:7) %>%
      filter(app=="TikTok")


#Convert format to long (category time), change values to match categories above
tiktok_timethisweek<-data_copy %>%
  select(wave, ResponseId,contains("app_time_by_name")) %>% 
  gather(key=category,value=tiktoktime,3:7) %>%
  mutate(category = case_when(
    category=="app_time_by_name_1" ~ "top_app_name_this_week_1",
    category=="app_time_by_name_2" ~ "top_app_name_this_week_2",
    category=="app_time_by_name_3" ~ "top_app_name_this_week_3",
    category=="app_time_by_name_4" ~ "top_app_name_this_week_4",
    category=="app_time_by_name_5" ~ "top_app_name_this_week_5"
  ))

#Get tiktok use only by participant and wave
tiktok_use <- left_join(tiktok_thisweek, tiktok_timethisweek, by=c("wave","ResponseId", "category")) %>%
  select(wave, ResponseId, category, app, tiktoktime)
  
#Convert tiktok use to minutes
  tiktok_use$tiktokmin = as.numeric(unlist(lapply(tiktok_use$tiktoktime, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(tiktok_use$tiktoktime, function (x) strsplit(x, ":")[[1]][2])))
  
  tiktok_use <- tiktok_use %>%
    select(wave, ResponseId, tiktokmin)
  
 
  
#Add social use & tiktok use column to data_copy
  data_copy <- left_join(data_copy, tiktok_use)
   data_copy$tiktokmin[is.na(data_copy$tiktokmin)] <- 0 #change NA for tiktok time to 0 so we can add it to social use
   data_copy <- left_join(data_copy, socialuse)
  
  
#Add tiktok use to social time, new column for screen time as proportion of total use
  data_copy <- data_copy %>%
    transform(socialtimemin_tik = socialtimemin + tiktokmin) %>%
      transform(social_proportion = socialtimemin_tik/tot_time_min)
  
  
  #'*Get social screen use LAST WEEK*
  
  #Convert format to long (category names)
socialmedialastweek<-data_copy %>%
  select(wave, ResponseId,contains("app_category_last_week_")) %>% 
  gather(key = category, value=app,3:7)


#Convert format to long (category time), change values to match categories above
socialmediatimelastweek<-data_copy %>%
  select(wave, ResponseId,contains("app_category_time_last_week_")) %>% 
  gather(key=category,value=socialtime_last,3:7) %>%
  mutate(category = case_when(
    category=="app_category_time_last_week_1" ~ "app_category_last_week_1",
    category=="app_category_time_last_week_2" ~ "app_category_last_week_2",
    category=="app_category_time_last_week_3" ~ "app_category_last_week_3",
    category=="app_category_time_last_week_4" ~ "app_category_last_week_4",
    category=="app_category_time_last_week_5" ~ "app_category_last_week_5"
  ))

#Join to get time per category, filter out non social networking values
socialuse_last<-left_join(socialmedialastweek, socialmediatimelastweek,by=c("wave","ResponseId", "category")) %>%
  filter(app=="Social Networking" | app=="social networking") %>%
     select("ResponseId", "socialtime_last")


#Convert time to numeric value (i.e. 2:04 to 124 minutes)
  socialuse_last$socialtimemin_last = as.numeric(unlist(lapply(socialuse_last$socialtime_last, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(socialuse_last$socialtime_last, function (x) strsplit(x, ":")[[1]][2])))
  
  
#All Screen Use:
  data_copy$tot_time_min_last = as.numeric(unlist(lapply(data_copy$tot_time_last_week, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(data_copy$tot_time_last_week, function (x) strsplit(x, ":")[[1]][2])))
  
#Average Use
    data_copy$avg_time_min_last = as.numeric(unlist(lapply(data_copy$avg_time_last_week, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(data_copy$avg_time_last_week, function (x) strsplit(x, ":")[[1]][2])))
    
    
#Add tiktok to social use
    #if tiktok in most used apps, extract matching time [top_app_name_this_week], [app_time_by_name]
    #add to social use
    
    tiktok_lastweek<-data_copy %>%
  select(wave, ResponseId,contains("top_app_name_last_week_")) %>% 
  gather(key = category, value=app,3:7) %>%
      filter(app=="TikTok")


#Convert format to long (category time), change values to match categories above
tiktok_timelastweek<-data_copy %>%
  select(wave, ResponseId,contains("app_time_by_name_last")) %>% 
  gather(key=category,value=tiktoktime,3:7) %>%
  mutate(category = case_when(
    category=="app_time_by_name_last_week_1" ~ "top_app_name_last_week_1",
    category=="app_time_by_name_last_week_2" ~ "top_app_name_last_week_2",
    category=="app_time_by_name_last_week_3" ~ "top_app_name_last_week_3",
    category=="app_time_by_name_last_week_4" ~ "top_app_name_last_week_4",
    category=="app_time_by_name_last_week_5" ~ "top_app_name_last_week_5"
  ))

#Get tiktok use only by participant and wave
tiktok_use_last <- left_join(tiktok_lastweek, tiktok_timelastweek, by=c("wave","ResponseId", "category")) %>%
  select(wave, ResponseId, category, app, tiktoktime)
  
#Convert tiktok use to minutes
  tiktok_use_last$tiktokmin_last = as.numeric(unlist(lapply(tiktok_use_last$tiktoktime, function (x) strsplit(x, ":")[[1]][1])))*60 +
  as.numeric(unlist(lapply(tiktok_use_last$tiktoktime, function (x) strsplit(x, ":")[[1]][2])))
  
  tiktok_use_last <- tiktok_use_last %>%
    select(wave, ResponseId, tiktokmin_last)
  
  #Add social use & tiktok use column to data_copy
  data_copy <- left_join(data_copy, tiktok_use_last)
   data_copy$tiktokmin_last[is.na(data_copy$tiktokmin_last)] <- 0 #change NA for tiktok time to 0 so we can add it to social use
   data_copy <- left_join(data_copy, socialuse_last, by="ResponseId")
  
  
#Add tiktok use to social time, new column for screen time as proportion of total use
  data_copy <- data_copy %>%
    transform(socialtimemin_tik_last = socialtimemin_last + tiktokmin_last) %>%
      transform(social_proportion_last = socialtimemin_tik_last/tot_time_min_last)
  


  
  
  #'*Make new column for final use (total, avg, total social) *
  #compare days represented for this_week and last_week, record which one has more days represented
    #if days represented in this week > days represented in last week & this week !NA
      #paste this week values into final column
    #else
      #paste last week values into final column
  #change NA for days represented to 0 so we can compare it
  data_copy$days_this_week[is.na(data_copy$days_this_week)] <- 0 
  data_copy$days_last_week[is.na(data_copy$days_last_week)] <- 0 
  #Choose which week of total time to use
  data_copy$final_total_min <- data_copy$tot_time_min  
  data_copy$final_total_min[(data_copy$days_this_week < data_copy$days_last_week) | is.na(data_copy$tot_time_min)] <- data_copy$tot_time_min_last[(data_copy$days_this_week < data_copy$days_last_week) | is.na(data_copy$tot_time_min)]  
  #Choose which week of average time to use
  data_copy$final_avg_min <- data_copy$avg_time_min
  data_copy$final_avg_min[(data_copy$days_this_week < data_copy$days_last_week)| is.na(data_copy$avg_time_min)] <- data_copy$avg_time_min_last[(data_copy$days_this_week < data_copy$days_last_week) | is.na(data_copy$avg_time_min)]
  #Choose which week of total social screen use (minutes) to use
  data_copy$final_social_min <- data_copy$socialtimemin_tik
  data_copy$final_social_min[(data_copy$days_this_week < data_copy$days_last_week)| is.na(data_copy$socialtimemin_tik)] <- data_copy$socialtimemin_tik_last[(data_copy$days_this_week < data_copy$days_last_week) | is.na(data_copy$socialtimemin_tik)]
  #Choose which week of social use proportion to use
  data_copy$final_social_prop <- data_copy$social_proportion
  data_copy$final_social_prop[(data_copy$days_this_week < data_copy$days_last_week)| is.na(data_copy$social_proportion)] <- data_copy$social_proportion_last[(data_copy$days_this_week < data_copy$days_last_week) | is.na(data_copy$social_proportion)]
  
  
  
  
```

Data Viz for Screentime
```{r}
#boxplot for social use by wave
ggboxplot(socialuse, x="wave", y="socialtimemin", color="wave")

#box plot for overall use by wave
ggboxplot(data_copy, x="wave", y="tot_time_min", color="wave")

#box plot for average overall use by wave
ggboxplot(data_copy, x="wave", y="avg_time_min", color="wave")

#box plot for proportion social use by wave
ggboxplot(processed_data, x="wave", y="social_proportion", color="wave")


packages <- c("ggplot2", "dplyr", "lavaan", "plyr", "cowplot", "rmarkdown", 
              "readr", "caTools", "bitops")

if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}
library(cowplot)
library(dplyr)
library(readr)

source("~/Documents/GitHub/RainCloudPlots/tutorial_R/R_rainclouds.R")
p_social_time <- ggplot(processed_data,aes(x=as.factor(wave),y=socialtimemin, fill = wave))+
  geom_flat_violin(position = position_nudge(x = .2, y = 0),adjust = 2)+
  geom_point(position = position_jitter(width = .15), size = .25)+
  ylab('Score')+xlab('Group')+coord_flip()+theme_cowplot()+guides(fill = FALSE)+
  ggtitle('Figure 3: The Basic Raincloud with Colour')

p_social_time

p_tot_time <- ggplot(processed_data,aes(x=as.factor(wave),y=tot_time_min, fill = wave))+
  geom_flat_violin(position = position_nudge(x = .2, y = 0),adjust = 2)+
  geom_point(position = position_jitter(width = .15), size = .25)+
  ylab('Score')+xlab('Group')+coord_flip()+theme_cowplot()+guides(fill = FALSE)+
  ggtitle('Figure 3: The Basic Raincloud with Colour')

p_tot_time


p_avg_time <- ggplot(processed_data,aes(x=as.factor(wave),y=avg_time_min, fill = wave))+
  geom_flat_violin(position = position_nudge(x = .2, y = 0),adjust = 2)+
  geom_point(position = position_jitter(width = .15), size = .25)+
  ylab('Score')+xlab('Group')+coord_flip()+theme_cowplot()+guides(fill = FALSE)+
  ggtitle('Figure 3: The Basic Raincloud with Colour')

p_avg_time

p_prop_social <- ggplot(processed_data,aes(x=as.factor(wave),y=social_proportion, fill = wave))+
  geom_flat_violin(position = position_nudge(x = .2, y = 0),adjust = 2)+
  geom_point(position = position_jitter(width = .15), size = .25)+
  ylab('Score')+xlab('Group')+coord_flip()+theme_cowplot()+guides(fill = FALSE)+
  ggtitle('Figure 3: The Basic Raincloud with Colour')

p_prop_social


#
```

Social Connections
```{r}
#Calculate percentage of similarly named friends for each wave
# Join netcanvas to all_data to get record_ids, select columns that we want to keep

netcanvas_by_record <- inner_join(netcanvas, all_data, by="ResponseId")%>%
  select(record_id, ResponseId, wave.x, name, household_relationship, illness, frequency_communication, give_support, receive_support, communication_methods, age) %>% # ~20 rows less than netcanvas...could be netcanvas ids missing from all_data
   filter(is.na(household_relationship)|household_relationship=="friend")

#Make new dataframe for counts
netcanvas_cleaned = test_set[FALSE,] #from testing below
netcanvas_cleaned <- netcanvas_cleaned  %>% 
  add_column(unique_friends = NA, .after = "names") %>% 
  add_column(common_friends = NA, .after = "unique_friends") %>%
  dplyr::rename(record_id=id)
netcanvas_cleaned$record_id <- as.integer(netcanvas_cleaned$record_id)


#For each kid, for each wave, split data by wave, iterate through waves, find intersect & union of friends in wave and wave+1, record info in new row of datatable

for (kid in unique(netcanvas_by_record$record_id)) {
  splitted <- split(netcanvas_by_record$name[which(netcanvas_by_record$record_id==kid)],netcanvas_by_record$wave.x[which(netcanvas_by_record$record_id==kid)])
  for(wave_num in 0:9){
     netcanvas_cleaned <- netcanvas_cleaned %>% add_row(record_id=kid, wave=wave_num+1, common_friends=length(intersect(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))), unique_friends=length(union(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))))
     #print(length(union(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))))
  }
}
#note: at wave, friends in common is 0, unique friends is number of friends at that wave
#note: when joining this back in, make sure we drop waves where the participant has no friends listed

#calculate proportion of friends stable between waves (common friends/unique friends)
netcanvas_cleaned <- netcanvas_cleaned %>%
transform(prop_friend_stability = common_friends / unique_friends)





save(netcanvas_cleaned, file="netcanvas_cleaned.Rda")

processed_data <- left_join(data_copy, netcanvas_cleaned)
processed_data <- processed_data %>%
  select(-c(names, category, app)) %>%
  filter(Finished == "True")

#Manually fix values for record ids
  processed_data[275,1]= 37
  processed_data[276,1]= 41
  processed_data[277,1]= 37
  processed_data[278,1]= 49
  processed_data[279,1]= 49
  processed_data[280,1]= 3
  processed_data[281,1]= 37
  processed_data[282,1]= 38
  processed_data[284,1]= 41
  
  processed_data <- processed_data %>%
    filter(!is.na(record_id)) 
    
    processed_data <- processed_data[order(processed_data$record_id, processed_data$wave),]


save(processed_data, file="processed_data.Rda")
```

Make Data Frame for Analysis
```{r}

analysis_data <- processed_data %>%
  #select(1:7, 99:100, 192:202, 216:222) #202, drop 210, 99:100
  select(1:9, 194:206, 221:227)
save(analysis_data, file="analysis_data.Rda")


#average stability over waves
avg_friend_stability <- analysis_data %>%
group_by(record_id) %>%
  summarize(avg_friend_stability = mean(na.omit(prop_friend_stability)))



```

Social Connections Plots
```{r}
#Boxplot
g = ggboxplot(processed_data, x="wave", y="prop_friend_stability", color="wave")
g


#Raincloud plot
p_stable_friends <- ggplot(processed_data,aes(x=as.factor(wave),y=prop_friend_stability, fill = wave))+
  geom_flat_violin(position = position_nudge(x = .2, y = 0),adjust = 2)+
  geom_point(position = position_jitter(width = .15), size = .25)+
  ylab('Score')+xlab('Group')+coord_flip()+theme_cowplot()+guides(fill = FALSE)+
  ggtitle('Figure 3: The Basic Raincloud with Colour')

p_stable_friends


#Alluvial Plots

#plot x as wave, y as count, alluvium as top app category?
processed_data$app_category_this_week_1[processed_data$app_category_this_week_1 == 'social networking'] <- 'Social Networking'
summarized_processed <- processed_data %>%
  count(wave, app_category_this_week_1) %>%
  filter(!is.na(app_category_this_week_1)) %>%
  filter(app_category_this_week_1!="")
summarized_processed$wave <- as.factor(summarized_processed$wave)
summarized_processed$app_category_this_week_1 <- as.factor(summarized_processed$app_category_this_week_1)



library(ggalluvial)
ggplot(summarized_processed,
       aes(x = wave, stratum = n, alluvium = app_category_this_week_1,
           fill = app_category_this_week_1, label = app_category_this_week_1)) +
  scale_fill_brewer(type = "qual", palette = "Set2") +
  geom_flow(stat = "alluvium", lode.guidance = "frontback",
            color = "darkgray") +
  geom_stratum() +
  theme(legend.position = "bottom") +
  ggtitle("top app categories across waves")

```

Intersect & For Loop Testing
```{r}
#intersect of id x wave


  #output should be num of names in common between 2 waves, maybe number unique names
  #mutate(common = intersect(group1, group2))%>%
  #mutate(unique = unique(group1, group2))
  #find ratio of names in common to total names
  #possibly use group_map 
  #lag()
  
  #where(wave = X)
  
  grouped<- netcanvas %>%
    group_by(ResponseId, wave.x) %>%
 filter(is.na(household_relationship)|household_relationship=="friend") %>%
    lapply(X=1:10,
        FUN = function(X){
          wave1=X
          wave2=X+1
          output<-group %>%
          intersect(wave1,wave2)
         }
      )
  
  net_list = split(netcanvas, netcanvas$ResponseId)
  
  data[data$participant %in% c(8, 10),]
  


#arrange by wave, group by id, intersect on group and lag(group)

#calculate percentage of similarly named friends across waves: percentage of what? total unique friends in both waves? friends in later waves? 

#for i in 1:length of group a(participant x wavea):
  #for z in 1:length of group b(participant x waveb)
    #if i==z
      #return true or +1

#find set union - need to figure out how to pass groups and get return value
#intersect(x,y)
a<- c('person1', 'person2', 'person3')
b<- c('person1', 'person4', 'person3')
intersect(a,b)
length(intersect(a,b))



#Test Data

id <- c("a", "a", "a", "b","b","b","b", "c", "c", "c", "d", "d", "d", "d", "a", "a","b","b","c","c","d","d")
wave<- c(1,1,2,1,1,2,2,1,2,2,1,1,2,1,3,3,3,3,3,3,3,3)
names<-c("amy", "arnold", "amy", "bob", "becca", "bob", "becca", "carl", "carly", "carl", "dan", "dani", "danny", "carly", "arnold", "amy", "becca", "bobby", "carly", "carl", "don", "dave")
test_set<- data.frame(id,wave,names) %>%
    group_by(id) %>%
  
  x<-c(1,2)
  loop_test<- #test_set%>%
  for(i in c(1,2)){
    function(i) print(intersect(test_set$names[which(wave==i)], test_set$names[which(wave==i+1)]))
  }
    
  loop_test<- test_set%>%
  lapply(x=1,
    FUN=function(x) intersect(test_set$names[which(wave==x)], test_set$names[which(wave==x+1)])
    )
  
  

  loop_test2<- test_set %>% group_map(intersect, test_set$names[which(wave==1)], test_set$names[which(wave==2)])
  
Reduce(intersect, split(test_set$names[which(wave==1)], test_set$wave[which(wave==1)]))
  intersect(split(test_set$names, test_set$wave), test_set$id)
  
fake <- test_set[which(test_set$wave==1),]# %>% select(-id)

for (kid in c('a','b','c','d') ) {
  print(Reduce(intersect, split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])))
}

for (kid in unique(id)) {   #,'b','c','d'
  splitted <- split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])
  for(wave in 1:2){
     print(length(intersect(unlist(splitted[wave]), unlist(splitted[wave+1]))))
  }
}


#This works! Wave 1 means 1 to 2, 2 means 2, 3, 3 is blank 
final = test_set[FALSE,]
final <- final  %>% add_column(friends = NA, .after = "names")
for (kid in unique(id)) {   #,'b','c','d'
  splitted <- split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])
  for(wave in 1:3){
    #friend_col[length(friend_col)+1] <- length(intersect(unlist(splitted[wave]), unlist(splitted[wave+1])))
    #final$friends[which(id==kid&wave==wave)] = length(intersect(unlist(splitted[wave]), unlist(splitted[wave+1])))
    #mutate(final$friends[which(id==kid&wave==wave)] = length(intersect(unlist(splitted[wave]), unlist(splitted[wave+1]))))
    #print(final$friends[which(id==kid&wave==wave)])
     final <- final%>% add_row(id=kid, wave=wave, friends=length(intersect(unlist(splitted[wave]), unlist(splitted[wave+1]))))
  }
}


    lapply(X=1:10,
        FUN = function(X){
          wave1=X
          wave2=X+1
          output<-group %>%
          intersect(wave1,wave2)
         }
      )

    # This works! 1 is zero, 2 is for wave 1->2, 3 is for 2->3
final = test_set[FALSE,]
final <- final  %>% add_column(common_friends = NA, .after = "names")
final <- final  %>% add_column(unique_friends = NA, .after = "names")

for (kid in unique(id)) {   #,'b','c','d'
  splitted <- split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])
  for(wave_num in 0:2){
     final <- final%>% add_row(id=kid, wave=wave_num+1, common_friends=length(intersect(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))))
  }
}
#add column for common_friends/unique friends



final = test_set[FALSE,]
final <- final  %>% add_column(common_friends = NA, .after = "names")
final <- final  %>% add_column(unique_friends = NA, .after = "names")

for (kid in unique(id)) {   #,'b','c','d'
  splitted <- split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])
  for(wave_num in 0:2){
     final <- final%>% add_row(id=kid, wave=wave_num+1, common_friends=length(intersect(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))), unique_friends =length(union(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))))
     print(length(union(unlist(splitted[wave_num]), unlist(splitted[wave_num+1]))))
  }
}
final <- final%>%
transform(prop_friend_stability = common_friends / unique_friends)







final_apply = test_set[FALSE,]
final_apply <- final_apply  %>% add_column(friends = NA, .after = "names")
for(kid in unique(id)){
  splitted <- split(test_set$names[which(id==kid)], test_set$wave[which(id==kid)])
  lapply(x=1:3, FUN=function(x)
           final_apply <- final_apply%>% add_row(id=kid, wave=x, friends=length(intersect(unlist(splitted[x]), unlist(splitted[x+1])))
         ))
}
               

```

Explore final data
```{r}
#Scatterplots per wave/variable
ggplot(filter(analysis_data, wave==10), aes(x = final_social_min, y = prop_friend_stability)) +
    geom_point()

ggplot(filter(analysis_data, wave==10), aes(x = final_social_prop, y = prop_friend_stability)) +
    geom_point()

#Plot variable over waves, line connecting each participant (Spaghetti plots)
ggplot(analysis_data, aes(x = wave, y = yam_total)) +  #questionable
  geom_line(aes(color = record_id)) 

#SPAGHETTI PLOTS
qplot(x=wave, y=yam_total, colour = factor(record_id),
data = analysis_data, geom = "line", main= "Anxiety Across Waves (per articipant)",xlab="Wave", ylab="YAM Score")  + theme(legend.position="none")
ggsave("AnxietyParticipants.png")

qplot(x=wave, y=mspss_total, colour = factor(record_id),
data = analysis_data, geom = "line", main="Social Support Across Waves (per participant)", xlab="wave", ylab="Social Support") + theme(legend.position="none")
ggsave("SocialSupportParticipants.png")

qplot(x=wave, y=prop_friend_stability, colour = factor(record_id),
data = analysis_data, geom = "line", main="Friend Stability Across Waves (per participant)", xlab="Wave", ylab="Friend Stability") + theme(legend.position="none")
ggsave("FriendStabilityParticipants.png")

qplot(x=wave, y=final_social_min, colour = factor(record_id),
data = analysis_data, geom = "line", main="Social Screen Minutes Across Waves (per participant)", xlab="Wave", ylab="Social Screen Minutes") + theme(legend.position="none")
ggsave("SocialMinutesParticipants.png")

qplot(x=wave, y=final_social_prop, colour = factor(record_id),
data = analysis_data, geom = "line", main="Social Screen Use Proportion Across Waves (per participant)", xlab="Wave", ylab="Social Screen Use Proportion") + theme(legend.position="none")
ggsave("SocialProportionsParticipants.png")

#GROUP AVERAGES OVER WAVES (PER AGE YEAR)
#aggregate
sample_sum <- analysis_data %>%
  group_by(wave, age_years) %>%
  summarize(yam_mean = mean(yam_total, na.rm=TRUE), mspss_mean=mean(mspss_total, na.rm=TRUE), social_min_mean=mean(final_social_min, na.rm=TRUE), social_prop_mean=mean(final_social_prop, na.rm=TRUE), total_min_mean=mean(final_total_min, na.rm=TRUE),
            total_avg_mean=mean(final_total_min, na.rm=TRUE), stability_mean=mean(prop_friend_stability, na.rm=TRUE),
            yam_sd=sd(yam_total, na.rm=TRUE), mspss_sd=sd(mspss_total, na.rm=TRUE), social_min_sd=sd(final_social_min, na.rm=TRUE), social_prop_sd=sd(final_social_prop, na.rm=TRUE), total_min_sd=sd(final_total_min, na.rm=TRUE),
            total_avg_sd=sd(final_total_min, na.rm=TRUE), stability_sd=sd(prop_friend_stability, na.rm=TRUE))%>%
  ungroup()
#
qplot(x=wave, y=yam_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Anxiety Across Waves (by age)", xlab="Wave", ylab="Anxiety") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=yam_mean-yam_sd, ymax=yam_mean+yam_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("AnxietyAge.png")

qplot(x=wave, y=mspss_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Social Support Across Waves (by age)", xlab="Wave", ylab="Social Support") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=mspss_mean-mspss_sd, ymax=mspss_mean+sd), width=.2,
                 position=position_dodge(0.05))
ggsave("SocialSupportAge.png")

qplot(x=wave, y=social_min_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Social Screen Minutes Across Waves (by age)", xlab="Wave", ylab="Social Screen Minutes") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=social_min_mean-social_min_sd, ymax=social_min_mean+social_min_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("SocialMinutesAge.png")

qplot(x=wave, y=social_prop_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Social Screen Proportion Across Waves (by age)", xlab="Wave", ylab="Social Screen Proportion") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=social_prop_mean-social_prop_sd, ymax=social_prop_mean+social_prop_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("SocialProortionAge.png")

qplot(x=wave, y=total_min_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Total Screen Minutes Across Waves (by age)", xlab="Wave", ylab="Total Screen Minutes") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=total_min_mean-total_min_sd, ymax=total_min_mean+total_min_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("TotalScreenMinAge.png")

qplot(x=wave, y=total_avg_mean, colour = factor(age_years),
data = sample_sum, geom = "line", main="Average Screen Minutes Across Waves (by age)", xlab="Wave", ylab="Average Screen Minutes") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=total_avg_mean-total_avg_sd, ymax=total_avg_mean+total_avg_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("AvgScreenMinAge.png")

qplot(x=wave, y=stability_mean, colour = factor(age_years),
data = sample_sum, geom = "line",  main="Friend Stability Across Waves (by age)", xlab="Wave", ylab="Friend Stability") + labs(colour = 'Age') +
    geom_errorbar(aes(ymin=stability_mean-stability_sd, ymax=stability_mean+stability_sd), width=.2,
                 position=position_dodge(0.05))
ggsave("FriendStabilityAge.png")


#Plot measures aligned by day of survey
```

Start Building Models
```{r}

# Models will include a random intercept for participant
# include month of data collection to account for differing restrictions/guidelines
# social use: total time & proportion of overall use
# Use lme4

#Model 1a: socially focused screen time & anxiety: yam_total ~ final_social_min, random=~1|record_id

#Model 1b: socially focused screen time & anxiety: yam_total ~ final_social_prop, random=~1|record_id

#Model 2a: socially focused screen time & perceived social support: mspss_total ~ final_social_min, random=~1|record_id

#Model 2b: socially focused screen time & perceived social support: mspss_total ~ final_social_prop, random=~1|record_id

#Model 3a: socially focused screen time & stable social connections: prop_friend_stability ~ final_social_min, random=~1|record_id

#Model 3b: socially focused screen time & stable social connections: prop_friend_stability ~ final_social_prop, random=~1|record_id
#Additional: Subscores (yam & mspss), interactions

#Template:
mod_Xab <- lme(~, random = ~1|record_id, data=analysis_data, na.action=na.omit)

fit <- lme(final_social_min ~ yam_total + mspss_total + prop_friend_stability, random = ~1|record_id, data=analysis_data, na.action=na.omit)
summary(fit) # show results

#Remove NAs and excluded participants (YAM & MSPSS)
##mspss_analysis <- subset(data_copy, (!(ResponseID %in% mspss_exclusion_ids)))
#yam_analysis <- subset(data_copy, (!(ResponseID %in% yam_exclusion_ids)))


```

Actual Models
```{r}
# We hypothesize that use of screen media, in particular screen media used for social engagement, provides a vital way for adolescents to connect with their friends and peers during periods of increased social isolation, such as the COVID-19 pandemic. Specifically, we think that elevated socially-focused screen media use may be related to a number of markers of well-being. We expect that anxiety will be lower and perceived social support will be higher in individuals with higher socially-focused screen media use. We also expect that individuals with more stable social ties have higher levels of socially-focused screen media use. 

# H1: yam_total ~ final_total_min compared to yam_total ~ final_total_min + final_social_min NEATER ALTERNATIVE: yam_total ~ final_social_prop 

mod_h1 <- lme(yam_total ~ final_total_min,
              random = ~1|record_id,
              data=(analysis_data %>% filter(!is.na(final_social_min))),
              method="ML",
              na.action=na.omit)

mod_h1_soc <- lme(yam_total ~ final_total_min + final_social_min,
                  random = ~1|record_id,
                  data=(analysis_data %>% filter(!is.na(final_social_min))),
                  method="ML",
                  na.action=na.omit)

anova(mod_h1,mod_h1_soc)

# now look at the proportional method, prob gonna see the same thing
mod_h1_prop <- lme(yam_total ~ final_social_prop,
              random = ~1|record_id,
              data=analysis_data,
              method="ML",
              na.action=na.omit)

# Now let's try the VAR approach for lagged relationships
var_h1 <- mlVAR(analysis_data,
                c("yam_total", "final_total_min"),
                "record_id",
                estimator="lmer",
                lags = 1,
                temporal = "fixed")
summary(var_h1)
plot(var_h1, "fixed", title = "Estimated temporal relationships", layout = "circle")

plot(var_h1, "contemporaneous", title = "Estimated contemporaneous relationships", 
    layout = "circle")

# H2: mspss_total ~ final_total_min compared to mspss_total ~ final_total_min + final_social_min

mod_h2 <- lme(mspss_total ~ final_total_min,
              random = ~1|record_id,
              data=(analysis_data %>% filter(!is.na(final_social_min))),
              method="ML",
              na.action=na.omit)

mod_h2_soc <- lme(mspss_total ~ final_total_min + final_social_min,
                  random = ~1|record_id,
                  data=(analysis_data %>% filter(!is.na(final_social_min))),
                  method="ML",
                  na.action=na.omit)

anova(mod_h2,mod_h2_soc)

#proportional method
mod_h2_prop <- lme(mspss_total ~ final_social_prop,
              random = ~1|record_id,
              data=analysis_data,
              method="ML",
              na.action=na.omit)

#lagged relationships
var_h2 <- mlVAR(analysis_data,
                c("mspss_total", "final_total_min"),
                "record_id",
                estimator="lmer",
                lags = 1,
                temporal = "fixed")
summary(var_h2)
plot(var_h2, "fixed", title = "Estimated temporal relationships", layout = "circle")

plot(var_h2, "contemporaneous", title = "Estimated contemporaneous relationships", 
    layout = "circle")

#H3: final_social_prop ~ prop_friend_stability

mod_h3 <- lme(final_social_min ~ prop_friend_stability,
              random = ~1|record_id,
              data=analysis_data,
              method="ML",
              na.action=na.omit)

mod_h3_prop <- lme(final_social_prop ~ prop_friend_stability,
              random = ~1|record_id,
              data=analysis_data,
              method="ML",
              na.action=na.omit)


var_h3 <- mlVAR(analysis_data,
                c("prop_friend_stability", "final_social_min"),
                "record_id",
                estimator="lmer",
                lags = 1,
                temporal = "fixed")
summary(var_h3)
plot(var_h3, "fixed", title = "Estimated temporal relationships", layout = "circle")

plot(var_h3, "contemporaneous", title = "Estimated contemporaneous relationships", 
    layout = "circle")


var_h3_prop <- mlVAR(analysis_data,
                c("prop_friend_stability", "final_social_prop"),
                "record_id",
                estimator="lmer",
                lags = 1,
                temporal = "fixed")
summary(var_h3_prop)
plot(var_h3_prop, "fixed", title = "Estimated temporal relationships", layout = "circle")

plot(var_h3_prop, "contemporaneous", title = "Estimated contemporaneous relationships", 
    layout = "circle")

```

